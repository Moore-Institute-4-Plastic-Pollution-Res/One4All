
# Functions ----
#' @title Generate a data frame with certificate information
#'
#' @description This function creates a data frame with certificate information including the current time,
#' data and rule hashes, package version, and web hash.
#'
#' @param x A list containing `data_formatted` and `rules` elements.
#' @param time the time the certificate is generated, can be passed a value or uses current system time.
#' @return A data frame with certificate information.
#' @importFrom digest digest
#' @importFrom shiny isTruthy
#' @importFrom utils packageVersion sessionInfo
#' @export
certificate_df <- function(x, time = Sys.time()){
    data.frame(time = time, 
                      data = digest(x$data_formatted), 
                      rules = digest(x$rules), 
                      package_version = paste(unlist(packageVersion("validate")), collapse = ".", sep = ""), 
                      web_hash = digest(paste(sessionInfo(), 
                                              Sys.time(), 
                                              Sys.info())))
}

#' Read rules from a file
#'
#' This function reads rules from a file or a data frame. 
#' The file can be in csv or xlsx format. 
#' The data should have the column names "name", "description", "severity", "rule".
#' The function also checks that the rules do not contain sensitive words and that 
#' all the rules fields are character type.
#'
#' @param file_rules The file containing the rules. Can be a CSV or XLSX file, or a data frame.
#'
#' @examples
#' read_rules("rules.csv") 
#' @return A data frame containing the rules.
#' @export
read_rules <- function(file_rules){
    # Reads the rules file.
    if(is.data.frame(file_rules)){
        rules <- file_rules
    }
    else{
        if(grepl("(\\.csv$)", ignore.case = TRUE, as.character(file_rules))){
            rules <- read.csv(file_rules)
        }
        else if(grepl("(\\.xlsx$)", ignore.case = TRUE, as.character(file_rules))){
            rules <- read_excel(file_rules)
        }
        else{
            stop('Uploaded rules format is not currently supported, please provide a rules file in csv or xlsx format.')  
        }
    }
    
    # Test that rules file has the correct required column names. 
    if (!all(c("name", "description", "severity", "rule") %in% names(rules))) {
        stop('Uploaded rules format is not currently supported, please provide a rules file with column names, "name", "description", "severity", "rule".')
    }
    
    # Tests that the rules do not contain sensitive words that may be malicious. 
    if (any(grepl("config|secret", rules$rule))) {
        stop('At this time we are unable to support any rules with the words "config" or "secret" in them as they could be malicious.')
    }
    
    # Tests that the rules severity is only warning or error
    if (!all(grepl("(error)|(warning)", rules$severity))) {
        stop('severity in the rules file can only be "error" or "warning"')
    }
    
    # Checks that all the rules fields are character type. 
    if (!all(sapply(rules, is.character))) {
        stop('Uploaded rules format is not currently supported, please provide a rules file with columns that are all character type.')
    }
    
    return(rules)
}

#' Read and format data from csv or xlsx files
#' 
#' @param files_data List of files to be read
#' @param data_names Optional vector of names for the data frames 
#' @importFrom readxl excel_sheets
#' @examples
#' ## You can add examples of function usage here.
#' 
#' @return A list of data frames
#' @export
read_data <- function(files_data, data_names = NULL){
    # Read in all csv files from files_data as a list. 
    if(is.list(files_data)){
        data_formatted <- files_data
    }
    else{
        if(all(grepl("(\\.csv$)", ignore.case = T, as.character(files_data)))){
            data_formatted <- tryCatch(lapply(files_data, function(x){read.csv(x)}),
                                       warning = function(w) {warning(w$message)}, 
                                       error = function(e) {stop(e$message)})
        }
        
        else if(all(grepl("(\\.xlsx$)", ignore.case = T, as.character(files_data)))){
            if(length(as.character(files_data)) > 1){
                data_formatted <- tryCatch(lapply(files_data, function(x){read_excel(x)}),
                                           warning = function(w) {warning(w$message)}, 
                                           error = function(e) {stop(e$message)})    
            }
            if(length(as.character(files_data)) == 1){
                sheets <- excel_sheets(files_data)
                data_formatted <- tryCatch(lapply(sheets, function(x){read_excel(files_data, sheet =  x)}),
                                           warning = function(w) {warning(w$message)}, 
                                           error = function(e) {stop(e$message)})    
            }
        }
        
        else{
            stop("You cannot mix data types, choose either csv or xlsx for all datasets.")
        }    
    }
    
    #Check if there is a warning when reading in the data. 
    if (inherits(data_formatted, "simpleWarning") | inherits(data_formatted, "simpleError")){
        stop(paste0("There was an error that said ", data_formatted$message))
    }
    
    #Names the data with the file names. 
    names(data_formatted) <- name_data(files_data, data_names)
    
    data_formatted 
}

#' Name datasets
#'
#' This function extracts the names of the datasets provided in the input files.
#' If specific data names are provided, they are used, otherwise the function tries
#' to extract the names from the files themselves.
#'
#' @param files_data A vector of file paths or list of data frames.
#' @param data_names A vector of names to be assigned to datasets.
#'
#' @return A vector of dataset names.
#' @examples
#' name_data(files_data = c("path/to/data1.csv", "path/to/data2.csv"))
#' name_data(files_data = c("path/to/data.xlsx"), data_names = c("sheet1", "sheet2"))
#' 
#' @importFrom readxl excel_sheets
#' 
#' @export
name_data <- function(files_data, data_names = NULL){
    #Grab the names of the datasets.
    if(isTruthy(data_names)){
        data_names <- gsub("(\\..*$)", "", gsub("(.*/)", "", data_names))
    } 
    else if(all(grepl("(\\.xlsx$)", ignore.case = T, as.character(files_data))) & length(as.character(files_data)) == 1){
        data_names <- excel_sheets(files_data)
    }
    else{
        data_names <- gsub("(\\..*$)", "", gsub("(.*/)", "", files_data))
    } 
    data_names
}


#' Reformats the rules
#'
#' This function is responsible for handling the rule reformating, dataset handling
#' and foreign key checks.
#'
#' @param rules A data.frame containing rules to be reformatted.
#' @param data_formatted A named list of data.frames with data.
#' @importFrom dplyr filter mutate bind_rows
#' @importFrom data.table rbindlist
#' @return A data.frame with reformatted rules.
#' @examples
#' reformat_rules(rules, data_formatted)
#' @export
reformat_rules <- function(rules, data_formatted, zip_data = NULL){
    #Add dataset if one doesn't exist so that everything else works. 
    if (!"dataset" %in% names(rules)){
        rules <- rules |>
            mutate(dataset = names(data_formatted))
    }
    
    #Check for special function checking additional files 
    rules <- rules |>
        dplyr::mutate(rule = ifelse(grepl("check_exists_in_zip(.*)", rule), 
                             paste0('check_exists_in_zip(zip_path = "', zip_data, '", file_name = ', gsub("(check_exists_in_zip\\()|(\\))", "", rule), ') == TRUE'), 
                             rule))
    
    #Circle back to add logic for multiple dfs
    #Check for special character "___" which is for assessing every column. 
    
    do_to_all <- rules |>
        dplyr::filter(grepl("___", rule))
    
    if(nrow(do_to_all) > 0){
        rules <- lapply(names(data_formatted), function(x){
            rules_sub <- do_to_all |> dplyr::filter(dataset == x)
            lapply(colnames(data_formatted[[x]]), function(new_name){
                rules_sub |>
                    dplyr::mutate(rule = gsub("___", new_name, rule)) |>
                    dplyr::mutate(name = paste0(new_name, "_", name))}) |>
                data.table::rbindlist()}) |>
            data.table::rbindlist() |>
            dplyr::bind_rows(rules |> dplyr::filter(!grepl("___", rule)))
    }
    
    # Check special character of is_foreign_key and if so then testing that foreign keys are exact. 
    foreign_keys <- rules |>
        dplyr::filter(grepl("is_foreign_key(.*)", rule))
    
    if(nrow(foreign_keys) > 0){
        columns <- gsub("(is_foreign_key\\()|(\\))", "", foreign_keys[["rule"]])

        rules <- lapply(1:nrow(foreign_keys), function(x){
            foreign_keys[x,] |>
                mutate(rule = paste0(columns[x], 
                                     ' %in% c("',
                                     paste(
                                         lapply(data_formatted, function(y){
                                             y[[columns[x]]]
                                         }) |> 
                                             unlist() |>
                                             unique(), 
                                         collapse = '", "', 
                                         sep = ""), 
                                     '")'))
        }) |>
            rbindlist() |>
            bind_rows(rules |> filter(!grepl("is_foreign_key(.*)", rule)))
    }
    rules
}


#' Validate_data: Validate data based on specified rules.
#'
#' @param files_data A list of file paths for the datasets to be validated.
#' @param data_names (Optional) A character vector of names for the datasets. If not provided, names will be extracted from the file paths.
#' @param file_rules A file path for the rules file, either in .csv or .xlsx format.
#'
#' @return A list containing the following elements:
#'   - data_formatted: A list of data frames with the validated data.
#'   - data_names: A character vector of dataset names.
#'   - report: A list of validation report objects for each dataset.
#'   - results: A list of validation result data frames for each dataset.
#'   - rules: A list of validator objects for each dataset.
#'   - status: A character string indicating the overall validation status ("success" or "error").
#'   - issues: A logical vector indicating if there are any issues in the validation results.
#'   - message: A data.table containing information about any issues encountered.
#'
#' @examples
#' # Validate data with specified rules
#' data("valid_example")
#' data("invalid_example")
#' data("test_rules")
#' 
#' result_valid <- validate_data(files_data = valid_example,
#'                         data_names = c("methodology", "particles", "samples"),
#'                         file_rules = test_rules)
#'                         
#' result_invalid <- validate_data(files_data = invalid_example,
#'                         data_names = c("methodology", "particles", "samples"),
#'                         file_rules = test_rules)
#'
#' @importFrom data.table data.table rbindlist
#' @importFrom readxl read_excel excel_sheets
#' @importFrom dplyr left_join mutate filter bind_rows across everything
#' @import validate
#' @importFrom shiny isTruthy
#' @importFrom utils read.csv
#' @export
validate_data <- function(files_data, data_names = NULL, file_rules = NULL, zip_data = NULL) {
    rules <- read_rules(file_rules)
    
    data_formatted <- read_data(files_data = files_data, data_names = data_names)
    
    if (!"dataset" %in% names(rules) & length(names(data_formatted)) > 1) {
        stop("If there is more than one dataset then a dataset column must be specified in the rules file to describe which rule applies to which dataset.")
    }
    
    if ("dataset" %in% names(rules)) {
        if (!all(unique(rules$dataset) %in% names(data_formatted))) {
            stop("If there is a dataset column in the rules file it needs to pertain to the names of the datasets being tested. The rules file lists datasets that do not match the datasets shared.")
        }
    }
    
    rules <- reformat_rules(rules = rules, data_formatted = data_formatted, zip_data = zip_data)
    
    rules_formatted <- tryCatch(validate::validator(.data=rules), 
                                warning = function(w) {
                                    warning(w)
                                    NULL
                                }, 
                                error = function(e) {
                                    stop(e)
                                })
    
    if (is.null(rules_formatted) || (length(class(rules_formatted)) != 1 || class(rules_formatted) != "validator")) {
        stop("There was an error with reading the rules file.")
    }
    
    all_variables <- unique(c(validate::variables(rules_formatted), unlist(lapply(data_formatted, names))))
    
    if (!(all(all_variables %in% validate::variables(rules_formatted)) & all(all_variables %in% unlist(lapply(data_formatted, names))))) {
        warning(paste0("All variables in the rules csv should be in the data csv and vice versa for the validation to work correctly. Ignoring these unmatched variables ", paste0(all_variables[!(all_variables %in% validate::variables(rules_formatted)) | !(all_variables %in% unlist(lapply(data_formatted, names)))], collapse = ", ")))
    }
    
    report <- lapply(names(data_formatted), function(x){
        validate::confront(data_formatted[[x]], validate::validator(.data=rules |> dplyr::filter(dataset == x))) 
    })
    
    results <- lapply(report, function(x) {
        validate::summary(x) |> 
            dplyr::mutate(status = ifelse(fails > 0 | error | warning , "error", "success")) |> 
            dplyr::left_join(rules)
    })
    
    any_issues <- vapply(results, function(x) {
        any(x$status == "error")
    }, FUN.VALUE = TRUE)
    
    rules_list_formatted <- tryCatch(lapply(names(data_formatted), function(x) {
        validator(.data=rules |> filter(dataset == x))
    }), 
    warning = function(w) {
        warning(w)
        NULL
    }, 
    error = function(e) {
        stop(e)
    })

list(data_formatted = data_formatted,
     data_names = data_names,
     zip_data = if(exists("zip_data")){zip_data} else{NULL},
     report = report, 
     results = results, 
     rules = rules_list_formatted, 
     issues = any_issues)
}

#' Remote Share Function
#'
#' This function uploads validated data to specified remote repositories,
#' such as CKAN and/or Amazon S3.
#'
#' @param validation A list containing validation information.
#' @param data_formatted A list containing formatted data.
#' @param verified The secret key provided by the portal maintainer.
#' @param valid_rules A list of valid rules for the dataset.
#' @param valid_key A list of valid keys.
#' @param ckan_url The URL of the CKAN instance.
#' @param ckan_key The API key for the CKAN instance.
#' @param ckan_package The CKAN package to which the data will be uploaded.
#' @param url_to_send The URL to send the data.
#' @param rules A set of rules used for validation.
#' @param results A list containing results of the validation.
#' @param s3_key_id AWS ACCESS KEY ID
#' @param s3_secret_key AWS SECRET ACCESS KEY
#' @param s3_region AWS DEFAULT REGION
#' @param s3_bucket The name of the Amazon S3 bucket.
#' @param old_cert (Optional) An old certificate to be uploaded alongside the new one to override the previous submission with.
#'
#' @return A list containing the status and message of the operation.
#' 
#' @importFrom data.table data.table fwrite
#' @importFrom digest digest
#' @importFrom ckanr ckanr_setup resource_create
#' @importFrom aws.s3 put_object
#' @importFrom utils write.csv read.csv
#' 
#' @examples
#' #Need to add. 
#' 
#' @export
remote_share <- function(validation, data_formatted, files, verified, valid_rules, valid_key, ckan_url, ckan_key, ckan_package, url_to_send, rules, results, s3_key_id, s3_secret_key, s3_region, s3_bucket, old_cert = NULL){
    
    use_ckan <- isTruthy(ckan_url) & isTruthy(ckan_key) & isTruthy(ckan_package)
    use_s3 <- isTruthy(s3_bucket)  
    
    if(!(use_ckan| use_s3)){
        stop("Upload will not work because no upload methods are specified.")
    }
    
    if(any(results$status == "error")){
        stop("There are errors in the dataset that persist. Until all errors are remedied, the data cannot be uploaded to the remote repository.")
    }
    
    if(!any(digest(as.data.frame(rules)) %in% valid_rules)){
        stop("If you are using a key to upload data to a remote repo then there must be a valid pair with the rules you are using in our internal database.")
    }
    
    if(!any(verified %in% valid_key)){
        stop("You must have a valid key provided by the portal maintainer to use this feature.")
    }

    hashed_data <- digest(data_formatted)
    
    structured_files <- paste(tempdir(), "\\", hashed_data, ".rds", sep = "")
    
    saveRDS(data_formatted, file = structured_files)
    
    submission_time <- as.double(as.POSIXlt(Sys.time(), "GMT"))
    
    if(use_ckan){
        ckanr_setup(url = ckan_url, key = ckan_key)
    }    
    
    if(use_s3){
        Sys.setenv(
            "AWS_ACCESS_KEY_ID" = s3_key_id,
            "AWS_SECRET_ACCESS_KEY" = s3_secret_key,
            "AWS_DEFAULT_REGION" = s3_region
        )
    }
    
    #Add certificate
    certificate_file <- tempfile(pattern = "certificate", fileext = ".csv")
    
    data.table::fwrite(certificate_df(validation, time = submission_time), certificate_file)
    
    files = c(files, structured_files, certificate_file)
    
    #Add old certificate
    if(isTruthy(old_cert)){
        files = c(files, old_cert)
    }
    
    temp_zip <- file.path(tempdir(), "temp.zip")
    
    zip(zipfile = temp_zip, files = files, extras = '-j') # Zip the test file
    
    hashed_zip = digest::digest(temp_zip)
    
            if(use_s3){
                put_object(
                    file = temp_zip,
                    object = paste0(hashed_zip, ".zip"),
                    bucket = s3_bucket
                ) 
            }
    
            if(use_ckan){
                resource_create(package_id = ckan_package,
                                description = "validated raw data upload to microplastic data portal",
                                name = paste0(hashed_zip, ".zip"),
                                upload = temp_zip)
            }
    
    message(paste0("Data was successfully sent to the data portal at ", url_to_send))

    return(list(hashed_zip = hashed_zip,
                submission_time = submission_time))
}

#' Download Data from Remote Sources
#'
#' This function downloads data from remote sources like CKAN, AWS S3.
#' It retrieves the data based on the hashed_data identifier and assumes the data is stored using the same naming conventions provided in the `remote_share` function.
#'
#' @param hashed_data A character string representing the hashed identifier of the data to be downloaded.
#' @param ckan_url A character string representing the CKAN base URL.
#' @param ckan_key A character string representing the CKAN API key.
#' @param ckan_package A character string representing the CKAN package identifier.
#' @param s3_key_id A character string representing the AWS S3 access key ID.
#' @param s3_secret_key A character string representing the AWS S3 secret access key.
#' @param s3_region A character string representing the AWS S3 region.
#' @param s3_bucket A character string representing the AWS S3 bucket name.
#'
#' @importFrom shiny isTruthy
#' @importFrom dplyr mutate_if
#' @importFrom aws.s3 get_bucket get_object save_object
#' @importFrom ckanr ckanr_setup package_show ckan_fetch
#' @importFrom readr read_rds
#' 
#' @return A named list containing the downloaded datasets.
#' 
#' @examples
#' \dontrun{
#'   downloaded_data <- remote_download(hashed_data = "example_hash",
#'                                      ckan_url = "https://example.com",
#'                                      ckan_key = "your_ckan_key",
#'                                      ckan_package = "your_ckan_package",
#'                                      s3_key_id = "your_s3_key_id",
#'                                      s3_secret_key = "your_s3_secret_key",
#'                                      s3_region = "your_s3_region",
#'                                      s3_bucket = "your_s3_bucket")
#' }
#' 
#' @export
remote_download <- function(hashed_zip = NULL, ckan_url, ckan_key, ckan_package, s3_key_id, s3_secret_key, s3_region, s3_bucket) {
    
    download_all <- !shiny::isTruthy(hashed_zip)
    use_ckan <- shiny::isTruthy(ckan_url) & shiny::isTruthy(ckan_key) & shiny::isTruthy(ckan_package)
    use_s3 <- shiny::isTruthy(s3_bucket)  
    
    if(use_ckan){
        ckanr::ckanr_setup(url = ckan_url, key = ckan_key)
    }
    
    if(use_s3){
        Sys.setenv(
            "AWS_ACCESS_KEY_ID" = s3_key_id,
            "AWS_SECRET_ACCESS_KEY" = s3_secret_key,
            "AWS_DEFAULT_REGION" = s3_region
        )
    }
    
    data_downloaded <- list()
    
    if(use_s3 & !download_all){
        # Retrieve a list of objects from S3 bucket
        s3_objects <- get_bucket(bucket = s3_bucket, prefix = hashed_zip) 
        file <- tempfile(fileext = ".zip")
        aws.s3::save_object(object = s3_objects[[1]]$Key, file = file, bucket = s3_bucket)
        zip_files <- unzip(file, list = TRUE)$Name
        structured_data <- zip_files[grepl(".rds$", zip_files)]
        unzip(file, files = structured_data)
        data_downloaded[["s3"]] <- read_rds(structured_data)
    }
    
    if(use_s3 & download_all){
        # Retrieve a list of objects from S3 bucket
        s3_objects <- get_bucket(bucket = s3_bucket) 
        for (obj in s3_objects) {
            if(grepl("^uploaded/", obj$Key)) next
            # Download each object
            file <- tempfile(fileext = ".csv")
            aws.s3::save_object(object = obj$Key, file = file, bucket = s3_bucket)
            
            # Read the data and store it in a named list
            dataset_name <- gsub("\\.csv$", "", obj$Key)
            data_downloaded[["s3"]][[dataset_name]] <- read.csv(file)
        }
    }
    
    if(use_ckan & !download_all){
        resources <- package_show(ckan_package)$resources
        resources_names <- vapply(resources, function(x){x$name}, FUN.VALUE = character(1))
        hashed_zip_resources <- resources[grepl(hashed_zip, resources_names)]
        file <- tempfile(fileext = ".zip")
        ckan_fetch(x = hashed_zip_resources[[1]]$url, store = "disk", path = file)
        zip_files <- unzip(file, list = TRUE)$Name
        structured_data <- zip_files[grepl(".rds$", zip_files)]
        unzip(file, files = structured_data)
        data_downloaded[["ckan"]] <- read_rds(structured_data)
    }
    
    if(use_ckan & download_all){
        resources <- package_show(ckan_package)$resources

        for (res in resources) {
            data <- ckan_fetch(x = res$url)
            dataset_name <- res$name
            data_downloaded[["ckan"]][[dataset_name]] <- data
        }
    }
    
    return(data_downloaded)
}

#' Check if an object is of class POSIXct
#'
#' This function checks if the given object is of class POSIXct.
#' It returns TRUE if the object inherits the POSIXct class, otherwise FALSE.
#'
#' @param x An object to be tested for POSIXct class inheritance.
#' @return A logical value indicating if the input object is of class POSIXct.
#' @examples
#' x <- as.POSIXct("2021-01-01")
#' is.POSIXct(x) # TRUE
#'
#' y <- Sys.Date()
#' is.POSIXct(y) # FALSE
#'
#' @export
is.POSIXct <- function(x) inherits(x, "POSIXct")

#' rules_broken ----
#'
#' Filter the results of validation to show only broken rules, optionally including successful decisions.
#'
#' @param results A data frame with validation results.
#' @param show_decision A logical value to indicate if successful decisions should be included in the output.
#'
#' @return A data frame with the filtered results.
#' @importFrom dplyr filter select everything
#' @export
#'
#' @examples
#' # Sample validation results data frame
#' sample_results <- data.frame(
#'   description = c("Rule 1", "Rule 2", "Rule 3"),
#'   status = c("error", "success", "error"),
#'   name = c("rule1", "rule2", "rule3"),
#'   expression = c("col1 > 0", "col2 <= 5", "col3 != 10"),
#'   stringsAsFactors = FALSE
#' )
#'
#' # Show only broken rules
#' broken_rules <- rules_broken(sample_results, show_decision = FALSE)
#' @export
rules_broken <- function(results, show_decision){
    results |>
        dplyr::filter(if(show_decision){status == "error"} else{status %in% c("error", "success")}) |>
        select(description, status, name, expression, everything())
}

#' rows_for_rules
#'
#' Get the rows in the data that violate the specified rules.
#'
#' @param data_formatted A formatted data frame.
#' @param report A validation report generated by the 'validate' function.
#' @param broken_rules A data frame with broken rules information.
#' @param rows A vector of row indices specifying which rules to check for violations.
#'
#' @return A data frame with rows in the data that violate the specified rules.
#' @importFrom validate violating validator confront
#' @export
#'
#' @examples
#' data("invalid_example")
#' data("test_rules")
#' # Generate a validation report
#' result_invalid <- validate_data(files_data = invalid_example,
#'                         data_names = c("methodology", "particles", "samples"),
#'                         file_rules = test_rules)
#'
#' # Find the broken rules
#' broken_rules <- rules_broken(results = result_invalid$results[[1]], show_decision = TRUE)
#'
#' # Get rows for the specified rules
#' violating_rows <- rows_for_rules(data_formatted = result_invalid$data_formatted[[1]], 
#'                                  report = result_invalid$report[[1]],
#'                                  broken_rules = broken_rules,
#'                                  rows = 2)
#' @export
rows_for_rules <- function(data_formatted, 
                           report, 
                           broken_rules, 
                           rows){
    violating(data_formatted, report[broken_rules[rows, "name"]])
}

#acknowledgement https://github.com/adamjdeacon/checkLuhn/blob/master/R/checkLuhn.R
#' Check if a number passes the Luhn algorithm
#'
#' This function checks if a given number passes the Luhn algorithm. It is commonly used to validate credit card numbers.
#' @param number A character string of the number to check against the Luhn algorithm.
#' @return A logical value indicating whether the number passes the Luhn algorithm (TRUE) or not (FALSE).
#' @examples
#' checkLuhn("4532015112830366") # TRUE
#' checkLuhn("4532015112830367") # FALSE
#' @export
checkLuhn <- function(number) {
    # must have at least 2 digits
    if(nchar(number) <= 2) {
        return(FALSE)
    }
    
    # strip spaces
    number <- gsub("-", "", gsub(pattern = " ", replacement = "", number))
    
    # Return FALSE if not a number
    if (!grepl("^[[:digit:]]+$", number)) {
        return(FALSE)
    }
    
    # split the string, convert it to a list, and reverse it
    digits <- unlist(strsplit(number, ""))
    digits <- digits[length(digits):1]
    
    to_replace <- seq(2, length(digits), 2)
    digits[to_replace] <- as.numeric(digits[to_replace]) * 2
    
    # gonna do some maths, let's convert it to numbers
    digits <- as.numeric(digits)
    
    # a digit cannot be two digits, so any that are greater than 9, subtract 9 and
    # make the world a better place
    digits <- ifelse(digits > 9, digits - 9, digits)
    
    # does the sum divide by 10?
    ((sum(digits) %% 10) == 0)
}

#' Check if a file exists in a zip file
#'
#' This function checks if a file with a given name exists in a specified zip file.
#' @param zip_path A character string representing the path of the zip file.
#' @param file_name A character string representing the name of the file to check.
#' @return A logical value indicating whether the file exists in the zip file (TRUE) or not (FALSE).
#' @importFrom utils unzip
#' @examples
#' # Example usage:
#' check_exists_in_zip(zip_path = "/path/to/your.zip", file_name = "file/in/zip.csv")
#' @export
check_exists_in_zip <- function(zip_path, file_name) {
    # List files in the zip
    zip_files <- unzip(zip_path, list = TRUE)$Name
    # Check if file_name is in the list of files
    file_exists <- file_name %in% zip_files
    return(file_exists)
}

#' Check and format image URLs
#'
#' This function checks if the input string contains an image URL (PNG or JPG) and formats it as an HTML img tag with a specified height.
#' @param x A character string to check for image URLs.
#' @return A character string with the HTML img tag if an image URL is found, otherwise the input string.
#' @examples
#' check_images("https://example.com/image.png")
#' check_images("https://example.com/image.jpg")
#' check_images("https://example.com/text")
#' @export
check_images <- function(x){
    ifelse(grepl("https://.*\\.png|https://.*\\.jpg", x), 
           paste0('<img src ="', x, '" height = "50"></img>'), 
           x)
}

#' Check and format non-image hyperlinks
#'
#' This function checks if the input string contains a non-image hyperlink and formats it as an HTML anchor tag.
#' @param x A character string to check for non-image hyperlinks.
#' @return A character string with the HTML anchor tag if a non-image hyperlink is found, otherwise the input string.
#' @examples
#' check_other_hyperlinks("https://example.com/page")
#' check_other_hyperlinks("https://example.com/image.png")
#' check_other_hyperlinks("https://example.com/image.jpg")
#' @export
check_other_hyperlinks <- function(x){
    ifelse(grepl("https://", x) & !grepl("https://.*\\.png|https://.*\\.jpg", x), 
           paste0('<a href ="', x, '">', x, '</a>'), 
           x)
}

#' Test for profanity in a string
#'
#' This function checks if the input string contains any profane words.
#' @param x A character string to check for profanity.
#' @return A logical value indicating whether the input string contains no profane words.
#' @import lexicon
#' @examples
#' test_profanity("This is a clean sentence.")
#' test_profanity("This sentence contains a badword.")
#' @export
test_profanity <- function(x){
    bad_words <- unique(tolower(c(#lexicon::profanity_alvarez, 
                                  #lexicon::profanity_arr_bad, 
                                  lexicon::profanity_banned#, 
                                  #lexicon::profanity_zac_anger#, 
                                  #lexicon::profanity_racist
                                  )))
    vapply(bad_words, function(y){
        !grepl(y, x, ignore.case = T)
    }, FUN.VALUE = T) |>
        all()
}

#Rules to excel
#' Create a formatted Excel file based on validation rules
#'
#' This function creates an Excel file with conditional formatting and data validation
#' based on the given validation rules in a CSV or Excel file.
#' @param file_rules A CSV or Excel file containing validation rules.
#' @param negStyle Style to apply for negative conditions (default is red text on a pink background).
#' @param posStyle Style to apply for positive conditions (default is green text on a light green background).
#' @param row_num Number of rows to create in the output file (default is 1000).
#' @param file_name Name of the output Excel file (default is "conditionalFormattingExample.xlsx").
#' @return A workbook object containing the formatted Excel file.
#' @importFrom readr read_csv
#' @importFrom readxl read_excel
#' @importFrom dplyr filter mutate bind_rows
#' @importFrom data.table rbindlist
#' @importFrom validate validator variables violating
#' @importFrom openxlsx createWorkbook addWorksheet writeData freezePane dataValidation conditionalFormatting saveWorkbook createStyle protectWorksheet
#' @importFrom tibble as_tibble tibble
#' @importFrom utils read.csv
#' @examples
#' data("test_rules")
#' create_valid_excel(file_rules = test_rules)
#' @export
create_valid_excel <- function(file_rules, 
                               negStyle  = createStyle(fontColour = "#9C0006", bgFill = "#FFC7CE"),
                               posStyle  = createStyle(fontColour = "#006100", bgFill = "#C6EFCE"),
                               row_num   = 1000){
    #Reads the rules file.
    if(is.data.frame(file_rules)){
        rules <- file_rules
    }
    else{
        if(grepl("(\\.csv$)", ignore.case = T, as.character(file_rules))){
            rules <- read.csv(file_rules)
        }
        
        if(grepl("(\\.xlsx$)", ignore.case = T, as.character(file_rules))){
            rules <- read_excel(file_rules)
        }
    }
    #Grab the names of the datasets.
    data_names <- if("dataset" %in% names(rules)){
        unique(rules$dataset)
    } 
    else{
        name <- gsub("(.*/)|(\\..*)", "", file_rules)
        rules$dataset <- name
        name
    }
    
    #Circle back to add logic for multiple dfs
    #Check for special character "___" which is for assessing every column. 
    
    do_to_all <- rules |>
        filter(grepl("___", .data$rule))
    
    if(nrow(do_to_all) > 0){
        rules <- lapply(data_names, function(x){
            rules_sub <- do_to_all |> filter(dataset == x)
            rules_sub_variables <- variables(validator(.data=rules_sub))
            lapply(rules_sub_variables, function(new_name){
                rules_sub |>
                    mutate(rule = gsub("___", new_name, rule)) |>
                    mutate(name = paste0(new_name, "_", name))
            }) |>
                rbindlist()
        }) |>
            rbindlist() |>
            bind_rows(rules |> filter(!grepl("___", rule)))
    }
    
    rules <- rules |>
        filter(!grepl("is_foreign_key(.*)", rule))
    
    lookup_column_index <- 1
    wb <- createWorkbook()
    addWorksheet(wb, "Rules")
    writeData(wb, sheet = "Rules", x = rules, startCol = 1)
    for(sheet_num in 1:length(data_names)){ #Sheet level for loop
        rules_all_raw <- rules |> filter(dataset == data_names[sheet_num])
        rules_all <- validator(.data = rules_all_raw)
        rule_variables <- variables(rules_all)
        sheet_name <- data_names[sheet_num]
        addWorksheet(wb, sheet_name)
        freezePane(wb, sheet_name, firstRow = TRUE) ## shortcut to freeze first row for every table.
        for(col_name in rule_variables){#Setup the column names with empty rows. 
            df <- as_tibble(rep("", row_num))
            names(df) <- col_name
            column_index_startup <- which(rule_variables == col_name)
            writeData(wb, sheet = sheet_name, x = df, startCol = column_index_startup)
        }
        for(col_num in 1:length(rules_all)){
            rule_test <- rules_all[[col_num]]
            expression <- rule_test@expr
            column_index <- which(rule_variables == variables(rule_test))
            if(any(grepl("(%vin%)|(%in%)", expression))){
                if(lookup_column_index == 1){
                    addWorksheet(wb, "Lookup")
                }
                values <- unlist(strsplit(gsub('(")|(\\))|(.*c\\()', "", as.character(expression[3])), ", "))
                lookup_col <- LETTERS[lookup_column_index] 
                df_lookup <- tibble(values)
                names(df_lookup) <- paste0(variables(rule_test), "_lookup")
                writeData(wb, 
                          sheet = "Lookup", 
                          x = df_lookup, 
                          startCol = lookup_column_index)
                dataValidation(wb, 
                               sheet = sheet_name, 
                               cols = column_index, 
                               rows = 2:row_num,
                               type = "list", 
                               value = paste0("Lookup!$", lookup_col, "$2:$", lookup_col, "$", length(values) +1))  
                lookup_column_index = lookup_column_index + 1
            }
            if(any(grepl("is_unique\\(.*\\)", expression))){
                conditionalFormatting(wb, 
                                      sheet_name, 
                                      cols = column_index, 
                                      rows = 2:row_num, 
                                      type = "duplicates", 
                                      style = negStyle)
            }
            if(sum(grepl("!|is.na(.*)", expression)) == 2){ #Not working yet.
                dataValidation(wb, 
                               sheet_name, 
                               cols = column_index, 
                               rows = 2:row_num, 
                               type = "textlength", 
                               operator = "greaterThanOrEqual",
                               value = "1",
                               allowBlank = FALSE)
            }
            if(any(grepl("in_range(.*)", expression))){
                dataValidation(wb, 
                               sheet_name, 
                               cols = column_index, 
                               rows = 2:row_num, 
                               type = "decimal", 
                               operator = "between",
                               value = c(as.numeric(as.character(expression)[grepl("^-|[0-9]+$", as.character(expression))][1]), 
                                         as.numeric(as.character(expression)[grepl("^-|[0-9]+$", as.character(expression))][2])))
            }
            if(any(grepl("grepl(.*)", expression))){ #could be improved with begins with and ends with logic.  
                good_conditions <- unlist(strsplit(gsub('(\\[[0-9]*-[0-9]*\\])|(\\])|(\\[)|(\\\\)|(\\^)|(\\$)|(\\))|(\\()', "",  as.character(expression)[2]), split = "\\|"))
                for(contain_condition in good_conditions){
                    conditionalFormatting(wb, 
                                          sheet_name, 
                                          cols = column_index, 
                                          rows = 2:row_num, 
                                          type = "contains",
                                          rule = contain_condition,
                                          style = posStyle)
                }
            }
            if(any(grepl("(%vin%)|(%in%)", expression))){
                protectWorksheet(
                    wb,
                    "Lookup",
                    protect = TRUE) #Protects the lookup table without a password just to prevent accidents.
            }
            #Need better way to deal with foreign keys, currently not working well. 
            
        }
    }
    wb
}



